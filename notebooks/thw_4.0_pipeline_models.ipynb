{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Models\n",
    "## From raw data to outputs in single pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "pd.set_option('display.max_columns',500)\n",
    "\n",
    "from IPython.display import display\n",
    "from scipy import stats\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\User\\Documents\\Programming Practice\\my_modules')\n",
    "\n",
    "from trav_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = '../data/raw/'\n",
    "data_interim = '../data/interim/'\n",
    "data_external = '../data/external/'\n",
    "data_processed = '../data/processed/'\n",
    "model_dir = '../models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\pyarrow\\pandas_compat.py:752: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels, = index.labels\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_feather(data_interim + 'train_01.ftr')\n",
    "df_test = pd.read_feather(data_interim + 'test_01.ftr')\n",
    "\n",
    "df_train = df_train.set_index(df_train.columns[0])\n",
    "df_test = df_test.set_index(df_test.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'Survived'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.drop(label,axis=1)\n",
    "y_train = df_train[label]\n",
    "\n",
    "X_test = df_test\n",
    "\n",
    "cols_to_drop = ['Cabin','Name']\n",
    "num_cols = X_train.select_dtypes(np.number).columns\n",
    "cat_cols = X_train.select_dtypes('category').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant',fill_value = 'missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "num_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers= [\n",
    "        ('cat',cat_transformer,cat_cols),\n",
    "        ('num',num_transformer,num_cols)]\n",
    ")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  50 | elapsed:    3.4s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    3.4s finished\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.2s finished\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 0.762 +/- 0.009\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Wall time: 4.18 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators':[20],\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_depth':stats.randint(2,100),\n",
    "    'min_samples_split':stats.randint(2,100),\n",
    "    'min_samples_leaf':stats.randint(1,100)\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    param_distributions,\n",
    "    n_iter = 10,\n",
    "    scoring = 'accuracy',\n",
    "    cv = 5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "scores = cross_val_score(\n",
    "    rs,\n",
    "    X_train_processed,\n",
    "    y_train,\n",
    "    scoring = 'accuracy',\n",
    "    cv = 2\n",
    ")\n",
    "\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n",
    "\n",
    "rs.fit(X_train_processed,y_train)\n",
    "results = pd.DataFrame(rs.cv_results_).sort_values('rank_test_score')\n",
    "\n",
    "model = rs.best_estimator_\n",
    "# display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions(model,X_test,X_test_processed):\n",
    "    predictions = (pd.DataFrame(model.predict(X_test_processed),index=X_test.index)\n",
    "                   .reset_index()\n",
    "                   .rename(columns = {0:'Survived'})\n",
    "                   .assign(Survived = lambda x: x['Survived'].astype(int))\n",
    "                  )\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = create_predictions(model,X_test,X_test_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model dataset, model, & results serialized together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write function which will save the model, model predictions .csv file, and the notebook used to create & run the model at that point in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_model(model,predictions,model_dir):\n",
    "    \"\"\"Output serialized model, predictions, and notebook used to make model/predictions\"\"\"\n",
    "    \n",
    "    for i in range(1,1000):\n",
    "        model_name = '{}model_{:03d}.pkl'.format(model_dir,i)\n",
    "        \n",
    "        # Check if filename exists, if not write files\n",
    "        if not os.path.isfile(model_name):\n",
    "            with open(model_name,'wb') as f:\n",
    "                pickle.dump(model,f)\n",
    "                \n",
    "            prediction_name = '{}predictions_{:03d}.csv'.format(model_dir,i)\n",
    "            predictions.to_csv(prediction_name,index=False)\n",
    "            \n",
    "            notebook_name = '{}notebook_{:03d}'.format(model_dir,i)\n",
    "            copy_current_nb(notebook_name)\n",
    "            return(prediction_name)\n",
    "    print('Error: More than 1000 models in folder')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_kaggle(competition,prediction_filename,message):\n",
    "    \"\"\"Submit prediction file to kaggle\"\"\"\n",
    "    !kaggle competitions submit -c $competition -f $prediction_filename -m $message\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_name = output_model(model,test_predictions,model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to Titanic: Machine Learning from Disaster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/3.18k [00:00<?, ?B/s]\n",
      "100%|##########| 3.18k/3.18k [00:00<00:00, 5.73kB/s]\n"
     ]
    }
   ],
   "source": [
    "submit_kaggle('titanic',prediction_name,'\\\"My second attempt at submitting a file via the kaggle command line interface\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
